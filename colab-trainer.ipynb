{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KrishPro/sentiment-analysis/blob/main/colab-trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwsCYpLUu7rN",
        "outputId": "8c986273-ab7f-45b8-9ee2-61a879fbc5de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PWD = \"/content/drive/MyDrive/Models/sentiment-analysis/bert\"\n",
        "# PWD = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "JrtXLZARu7rR",
        "outputId": "69187a03-c468-4c43-a0ff-356ea1e6eeaf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-699c454405c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-699c454405c4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-699c454405c4>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spacy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"[CLS]\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-699c454405c4>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, data_dir)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0msplits\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfile_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: listdir: path should be string, bytes, os.PathLike, integer or None, not functools.partial"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Written by KrishPro @ KP\n",
        "\"\"\"\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import Vocab, vocab as build_vocab\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.utils.data as data\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import re\n",
        "\n",
        "PAD_IDX = 0\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, batch_size: int) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        tqdm.pandas()\n",
        "\n",
        "        self.tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
        "\n",
        "        df: pd.DataFrame = self.load_data(self.tokenizer)\n",
        "\n",
        "        df['text'] = df['text'].map(lambda t: [\"[CLS]\"] + t)\n",
        "\n",
        "        self.vocab = self.create_vocab(df['text'], min_freq=2)\n",
        "        \n",
        "        df['text'] = df['text'].apply(self.vocab)\n",
        "\n",
        "        df = df[df['text'].map(len) < 500]\n",
        "\n",
        "        df['text_len'] = df['text'].map(len)\n",
        "\n",
        "        df = df.sort_values('text_len')\n",
        "        df = df.drop('text_len', axis=1).reset_index().drop('index', axis=1)\n",
        "        \n",
        "        self.data = df.values.tolist()\n",
        "        self.data = list(self.chunks(self.data, batch_size))[:-1]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rev, label = self.data[idx]\n",
        "        rev = [torch.tensor(r) for r in rev]\n",
        "        label = torch.tensor(label)\n",
        "        return pad_sequence(rev, padding_value=PAD_IDX), label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    @staticmethod\n",
        "    def chunks(lst, n):\n",
        "        \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "        for i in range(0, len(lst), n):\n",
        "            yield [l[0] for l in lst[i:i + n]], [l[1] for l in lst[i:i + n]]\n",
        "\n",
        "    def load_data(self, data_dir: str = \"/home/krish/Datasets/IMDB-Pos-vs-Neg\"):\n",
        "        if os.path.exists(os.path.join(PWD, \"Data/data.json\")):\n",
        "            return pd.read_json(os.path.join(PWD, \"Data/data.json\"))\n",
        "\n",
        "        else:\n",
        "            splits: list[str] = os.listdir(data_dir)\n",
        "\n",
        "            file_paths = [os.path.join(data_dir, file_name) for file_name in splits]\n",
        "            dataframe = pd.concat(list(map(pd.read_csv, file_paths)), ignore_index=True)\n",
        "            dataframe['text'] = dataframe['text'].map(self.clean_text)\n",
        "            dataframe['text'] = dataframe['text'].progress_map(self.tokenizer)\n",
        "            return dataframe\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_text(text: str) -> str:\n",
        "        text = text.lower()\n",
        "        text = text.replace(\"<br />\", \"\")\n",
        "        text = re.sub(r'[^\\w\\s]', \"\", text)\n",
        "        return text\n",
        "\n",
        "    def create_vocab(self, text: pd.Series, min_freq: int) -> Vocab:\n",
        "        if os.path.exists(os.path.join(PWD, \"Data/word-count.pth\")):\n",
        "            counter = torch.load(os.path.join(PWD, \"Data/word-count.pth\"))\n",
        "        else:\n",
        "            counter = Counter()\n",
        "            for t in tqdm(text):\n",
        "                counter.update(t)\n",
        "            torch.save(counter, os.path.join(PWD, \"Data/word-count.pth\"))\n",
        "\n",
        "\n",
        "        vocab = build_vocab(counter, min_freq)\n",
        "        vocab.insert_token(\"[UNK]\", len(vocab))\n",
        "        vocab.insert_token(\"[PAD]\", PAD_IDX)\n",
        "        vocab.set_default_index(vocab.get_stoi()[\"[UNK]\"])\n",
        "\n",
        "        return vocab\n",
        "\n",
        "def main():\n",
        "    dataset = Dataset(64)\n",
        "    dataloader = data.DataLoader(dataset, batch_size=None, shuffle=True, num_workers=os.cpu_count(), pin_memory=True)\n",
        "\n",
        "    rev, tar = next(iter(dataloader))\n",
        "\n",
        "    print(rev.shape, tar.shape)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbJR1rklu7rU"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Written by KrishPro @ KP\n",
        "\"\"\"\n",
        "\n",
        "from typing import Callable, Union\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import math\n",
        "\n",
        "# `PositionalEncoding` is copied from https://pytorch.org/tutorials/beginner/translation_transformer.html#seq2seq-network-using-transformer\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: torch.Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "class EmbeddingLayer(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size: int, dropout: float, padding_idx: int):\n",
        "        super(EmbeddingLayer, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, indices: torch.Tensor):\n",
        "        assert indices.dtype == torch.long, f\"indices.dtype must be torch.long, Got {indices.dtype}\"\n",
        "        \n",
        "        embeddings: torch.Tensor = self.embedding_layer(indices) * math.sqrt(self.emb_size)\n",
        "        embeddings: torch.Tensor = self.positional_encoding(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "class Bert(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int, nhead: int, dim_feedforward: int, num_encoder_layers: int, dropout: float, padding_idx: int, activation:  Union[str, Callable[[torch.Tensor], torch.Tensor]] = F.relu, layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False):\n",
        "        super(Bert, self).__init__()\n",
        "\n",
        "        self.pad_idx = padding_idx\n",
        "        self.embedding_layer = EmbeddingLayer(vocab_size, d_model, dropout, self.pad_idx)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
        "                                                    activation, layer_norm_eps, batch_first, norm_first)\n",
        "        encoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
        "\n",
        "        self.classifier = nn.Linear(d_model, 1)\n",
        "\n",
        "    def create_pad_mask(self, r: torch.Tensor):\n",
        "        return (r == self.pad_idx).T\n",
        "\n",
        "    def forward(self, r: torch.Tensor):\n",
        "        # r.shape: (S, N)\n",
        "        padding_mask: torch.Tensor = self.create_pad_mask(r)\n",
        "        r: torch.Tensor = self.embedding_layer(r)\n",
        "        # r.shape: (S, N, E)\n",
        "        mem: torch.Tensor = self.encoder(r, mask=None, src_key_padding_mask=padding_mask)\n",
        "        # mem.shape: (S, N, E)\n",
        "        mem = mem[0] # Taking the encoding for the [CLS] token\n",
        "        # mem.shape: (N, E)\n",
        "        output: torch.Tensor = self.classifier(mem)\n",
        "        # output.shape: (N, 1)\n",
        "        return torch.sigmoid(output).squeeze(1)\n",
        "\n",
        "def main():\n",
        "    bert = Bert(d_model=512, vocab_size=30_000, nhead=8, dim_feedforward=512, num_encoder_layers=6, dropout=0.1, padding_idx=0)\n",
        "\n",
        "    fake_reviews = torch.randint(0, 29_999, (150, 64))\n",
        "    fake_outs: torch.Tensor = bert(fake_reviews)\n",
        "\n",
        "    print(fake_reviews.shape, fake_outs.shape)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L73u2JaIu7rV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Written by KrishPro @ KP\n",
        "\"\"\"\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils import data\n",
        "from data import PAD_IDX, Dataset\n",
        "from model import Bert\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "\n",
        "LEARNING_RATE = 3e-4\n",
        "BATCH_SIZE = 64\n",
        "D_MODEL = 512\n",
        "NHEAD = 8\n",
        "NUM_ENCODER_LAYERS = 6\n",
        "DIM_FEEDFORWARD = D_MODEL * 2\n",
        "DROPOUT = 0.1\n",
        "\n",
        "def load_checkpoint(bert: Bert, checkpoint_path: str = \"checkpoints/latest.pth\"):\n",
        "    checkpoint_path = os.path.join(PWD, checkpoint_path)\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        bert.load_state_dict(torch.load(checkpoint_path))\n",
        "    return bert\n",
        "\n",
        "def save_checkpoint(bert: Bert, checkpoint_path: str = \"checkpoints/latest.pth\"):\n",
        "    checkpoint_path = os.path.join(PWD, checkpoint_path)\n",
        "    torch.save(bert.state_dict(), checkpoint_path)\n",
        "\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    dataset = Dataset(BATCH_SIZE)\n",
        "    dataloader = data.DataLoader(dataset, batch_size=None, shuffle=True, num_workers=os.cpu_count(), pin_memory=True)\n",
        "\n",
        "    vocab_size = len(dataset.vocab)\n",
        "\n",
        "    bert = Bert(D_MODEL, vocab_size, NHEAD, DIM_FEEDFORWARD, NUM_ENCODER_LAYERS, DROPOUT, PAD_IDX)\n",
        "    bert = load_checkpoint(bert)\n",
        "    bert = bert.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(bert.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n",
        "    criterion = nn.BCELoss() \n",
        "\n",
        "    writer = SummaryWriter(os.path.join(PWD, f\"runs/{time.time()}\"))\n",
        "    global_step = 0\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with tqdm(dataloader) as pbar:\n",
        "        for i, (review, target) in enumerate(pbar):\n",
        "\n",
        "            predictions = bert(review.to(device))\n",
        "\n",
        "            loss: torch.Tensor = criterion(predictions, target.to(device).float())\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            writer.add_scalar(\"loss\", loss.item(), global_step=global_step)\n",
        "            global_step += 1\n",
        "\n",
        "            if i % 4 == 0:\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "        \n",
        "        save_checkpoint(bert)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "51c3027d29855e291c508a403bd95dc5b3bd5d8a1e04d531ccc56fa9e3806d4b"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 ('basics')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "colab-trainer.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}