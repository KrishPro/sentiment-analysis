{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7a33d-03a2-495b-a687-4d0c6eaba614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from transformers.models.auto.modeling_auto import AutoModel\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "from transformers.models.auto.configuration_auto import AutoConfig\n",
    "from typing import Optional\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2942af33-438b-48d5-a4ff-e01b499c4869",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f33e14-8af9-4ec5-83e6-02bb4a7357b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    def __init__(self, split, data_dir):\n",
    "        self.df = pd.read_csv(os.path.join(data_dir, f'{split}_data.csv'))\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "    def __getitem__(self, idx: int):\n",
    "        return tuple(self.df.iloc[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    @classmethod\n",
    "    def collate_fn(cls, data):\n",
    "        sentences, sentiments = zip(*data)\n",
    "        sentences = cls.tokenizer(list(sentences), padding=True, return_tensors=\"pt\")\n",
    "        sentiments = torch.tensor(list(sentiments), dtype=torch.float32)\n",
    "        return sentences, sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b3fef-1efc-4413-963c-af8e5486d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "  \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        self.train_dataset = Dataset(\"train\", self.data_dir)\n",
    "        self.val_dataset  = Dataset(\"test\", self.data_dir)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=Dataset.collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=Dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d92c610-152c-4735-a25b-e38df34fedd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer = AutoModel.from_pretrained(model_name, add_pooling_layer=False)\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        \n",
    "        self.pooler = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(768, 384),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(384, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, sentences: dict[str, torch.Tensor]):\n",
    "        cls_token = self.transformer(**sentences).last_hidden_state[:, 0, :]\n",
    "        sentiment = self.pooler(cls_token)\n",
    "        return sentiment.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c7c66-2f46-4df7-aca4-e93c6db94d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    \"\"\"\n",
    "    This class contains all the functions required for training...\n",
    "    \"\"\"\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    transformer_lr = 1e-5\n",
    "    pooler_lr = 3e-4\n",
    "    \n",
    "    weight_decay = 0.02\n",
    "    default_lr = 3e-4\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        sentences, true_sentiments = batch\n",
    "        pred_sentiments = self(sentences)\n",
    "        loss = Train.criterion(pred_sentiments, true_sentiments)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        sentences, true_sentiments = batch\n",
    "        pred_sentiments = self(sentences)\n",
    "        loss = Train.criterion(pred_sentiments, true_sentiments)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        transformer = {\n",
    "            'decay': list(map(lambda s: s[1], filter(lambda s: s[0].endswith('weight'), self.transformer.named_parameters()))),\n",
    "            'no_decay': list(map(lambda s: s[1], filter(lambda s: s[0].endswith('bias'), self.transformer.named_parameters()))),\n",
    "            'weight_decay': Train.weight_decay,\n",
    "            'learning_rate': Train.transformer_lr\n",
    "        }\n",
    "        \n",
    "        pooler = {\n",
    "            'decay': list(map(lambda s: s[1], filter(lambda s: s[0].endswith('weight'), self.pooler.named_parameters()))),\n",
    "            'no_decay': list(map(lambda s: s[1], filter(lambda s: s[0].endswith('bias'), self.pooler.named_parameters()))),\n",
    "            'weight_decay': Train.weight_decay,\n",
    "            'learning_rate': Train.pooler_lr\n",
    "        }\n",
    "        \n",
    "        params = [\n",
    "            {'params': transformer['decay'], 'lr': transformer['learning_rate'], 'weight_decay': transformer['weight_decay']},\n",
    "            {'params': transformer['no_decay'], 'lr': transformer['learning_rate'], 'weight_decay': 0.0},\n",
    "            \n",
    "            {'params': pooler['decay'], 'lr': pooler['learning_rate'], 'weight_decay': pooler['weight_decay']},\n",
    "            {'params': pooler['no_decay'], 'lr': pooler['learning_rate'], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        \n",
    "        return optim.Adam(params, lr=Train.default_lr)\n",
    "    \n",
    "    \n",
    "Model.training_step = Train.training_step\n",
    "Model.validation_step = Train.validation_step\n",
    "Model.configure_optimizers = Train.configure_optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c2f645-05e0-423c-be31-0b328a03ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = DataModule(\"Data/\", batch_size=8)\n",
    "model = Model(model_name)\n",
    "trainer = pl.Trainer(gpus=1, accumulate_grad_batches=8, max_epochs=50, callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e0529-416f-46cc-b773-f0427501f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
